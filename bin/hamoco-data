#!/usr/bin/env python

#TODO: Add an option '--save_snapshots'
#TODO: Add an option '--reset' to remove all saved poses before recording
#TODO: Start the `snapshot_index` variable at the last value if saved files already exist
#TODO: Add a global description to the parser
#TODO: Add a way to stop the recording (e.g. provide the max number of snapshots)

import os
import time
import argparse

import cv2
import mediapipe as mp
from hamoco import Hand, HandSnapshot

# Mediapipe shortcuts
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

# All hand poses names
hand_poses = []
for pose in Hand.Pose:
    hand_poses.append(pose.name)
hand_poses.remove('UNDEFINED')
hand_poses_listed = ', '.join([f'"{pose}"' for pose in hand_poses])

# Parser
parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('pose',
                    type=str,
                    help=f'Hand pose to record (should be one of {hand_poses_listed})')
parser.add_argument('path_to_data',
                    type=str,
                    help='Path to the directory that will store the recorded data')
parser.add_argument('--delay',
                    type=float,
                    default=0.5,
                    help='Delay between consecutives snapshots')
parser.add_argument('-i', '--images',
                    action='store_true',
                    help='Path to the directory that will store the recorded data')
parser.add_argument('-t', '--test',
                    action='store_true',
                    help='Do not save the data and only show real-time information about hand detection')
args = parser.parse_args()
# Custom variables linked to parser
pose_name = args.pose
path_to_data = args.path_to_data
delay_between_snapshots = args.delay
save_images = args.images
record = not(args.test)

# Track snapshots
last_snapshot = time.time()
snapshot_index = 0
training_pose = Hand.Pose[pose_name]

# Webcam input
capture = cv2.VideoCapture(0)
with mp_hands.Hands(static_image_mode=False,
                    model_complexity=1,
                    max_num_hands=1,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5) as hands:

    # Detect hand movement while the video capture is on
    while capture.isOpened():
        success, image = capture.read()
        image_height, image_width, _ = image.shape

        if not success:
            print("Ignoring empty camera frame.")
            # If loading a video, use 'break' instead of 'continue'.
            continue

        # To improve performance, optionally mark the image as not writeable to
        # pass by reference.
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.flip(image, 1)
        results = hands.process(image)

        # Draw the hand annotations on the image.
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    image,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style())

        # Display the image (stop if ESC key is pressed)
        cv2.imshow('MediaPipe Hands', image)
        if cv2.waitKey(5) & 0xFF == 27:
            break

        # Snapshot every `delay_between_snapshot` seconds
        if time.time() - last_snapshot > delay_between_snapshots:

            if results.multi_hand_landmarks:
                landmarks = results.multi_hand_landmarks[0].landmark

                if record:
                    hand = Hand(pose=training_pose)
                    snapshot = HandSnapshot(hand=hand)
                    output_path = os.path.join(path_to_data, f'snapshot_{snapshot_index:04}_pose-{hand.pose.value}-{hand.pose.name}')
                    snapshot.save_landmarks_vector(landmarks, path=output_path)
                    if save_images:
                        snapshot.save_processed_image(image, path=output_path)
                    
                    saved_at = time.strftime('%H:%M:%S')
                    print(f'# Saved snapshot #{snapshot_index} for pose "{pose_name}" ({saved_at})')
                    last_snapshot = time.time()
                    snapshot_index += 1

                else:
                    which_hand = results.multi_handedness[0].classification[0].label
                    confidence = int(100 * results.multi_handedness[0].classification[0].score)
                    print('Hand detected={} | Confidence={}%'.format(which_hand, confidence))
