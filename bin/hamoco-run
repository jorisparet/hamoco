#!/usr/bin/env python

#TODO: adapt certain elements to the current framerate
#TODO: more accurate mouse movement with close fist
#TODO: relative pointer movement instead of absolute
#TODO: struggle to go from open to index up?
#TODO: put all the controller-related actions somewhere else
#TODO: Add a global description to the parser

import time
import argparse

import cv2
import mediapipe as mp
import pyautogui
import keras
import numpy

from hamoco import Hand, HandyMouseController
from hamoco.models import __default_model__

# Mediapipe shortcuts
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

# Allow mouse to go on the border of the screen
pyautogui.FAILSAFE = False

# Parser
parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('-S', '--sensitivity', 
                    default=0.25, 
                    type=float, 
                    help='Mouse sensitivity (between 0 and 1)')
parser.add_argument('--scrolling_threshold', 
                    default=0.1, 
                    type=float, 
                    help='Amount of change from the origin scrolling position to start scrolling (between 0 and 1)')
parser.add_argument('--min_cutoff_filter', 
                    default=0.1, 
                    type=float, 
                    help='Minimum cutoff for motion smoothing (low values decrease jitter but increase speed lag)')
parser.add_argument('--minimum_prediction_confidence',
                    default=0.8, 
                    type=float, 
                    help='Minimum prediction confidence for predicting hand poses')
parser.add_argument('-m', '--model', 
                    default=None, 
                    type=str,
                    help='Path to the Keras model to use for hand pose prediction')
parser.add_argument('--show',
                    action='store_true',
                    help='Real-time display of the processed camera feed')
args = parser.parse_args()
# Custom variables linked to parser
sensitivity = args.sensitivity
min_cutoff_filter = args.min_cutoff_filter
scrolling_threshold = args.scrolling_threshold
minimum_prediction_confidence = args.minimum_prediction_confidence
model = args.model
show_feed = args.show

# Load classification model
path_to_model = __default_model__ if model is None else model
trained_model = keras.models.load_model(path_to_model)

# Hand controller
hand_controller = HandyMouseController(sensitivity=sensitivity,
                                       scrolling_threshold=scrolling_threshold,
                                       min_cutoff_filter=min_cutoff_filter)

# Webcam input
capture = cv2.VideoCapture(0)
with mp_hands.Hands(static_image_mode=False,
                    model_complexity=1,
                    max_num_hands=1,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5) as hands:

    # Detect hand movement while the video capture is on
    while capture.isOpened():
        success, image = capture.read()
        image_height, image_width, _ = image.shape
        image = cv2.flip(image, 1)

        if not success:
            print('Ignoring empty camera frame.')
            # If loading a video, use 'break' instead of 'continue'.
            continue

        # A hand is detected
        results = hands.process(image)
        if results.multi_hand_landmarks:

            # Draw the hand annotations on the image
            # We only look at the first detected hand
            image.flags.writeable = True
            mp_drawing.draw_landmarks(image, 
                results.multi_hand_landmarks[0], 
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())

            # Landmark coordinates of the detected hand
            landmarks = results.multi_hand_landmarks[0].landmark
            hand = Hand()
            raw_landmark_vector = hand.vectorize_landmarks(landmarks)
            palm_center = hand_controller.palm_center(raw_landmark_vector)

            # Predict hand pose
            processed_landmark_vector = hand.feature_process_landmarks(raw_landmark_vector)
            probabilities = trained_model.predict(processed_landmark_vector).flatten()
            prediction_confidence = numpy.max(probabilities)
            predicted_pose = numpy.argmax(probabilities)
            hand.pose = Hand.Pose(predicted_pose)

            # Perform the appropriate mouse action
            hand_controller.operate_mouse(hand, 
                                          palm_center,
                                          prediction_confidence,
                                          min_confidence=minimum_prediction_confidence)

        # Show the camera feed
        if show_feed:
            # Hand center
            if results.multi_hand_landmarks:
                cursor_xy = (palm_center * numpy.array([image_width, image_height])).astype(int)
                cursor_size = 10
                cv2.rectangle(image, tuple(cursor_xy - cursor_size), 
                                    tuple(cursor_xy + cursor_size), 
                                    (255,255,255), 3)
            # Accessible area        
            xmin, ymin, xmax, ymax = hand_controller.accessible_area(image)
            hidden = numpy.zeros_like(image, numpy.uint8)
            alpha = 0.25
            color = (1,1,1)
            cv2.rectangle(hidden, (0, 0), (xmin, image_height), color, -1)
            cv2.rectangle(hidden, (xmin, 0), (xmax, ymin), color, -1)
            cv2.rectangle(hidden, (xmax, 0), (image_width, image_height), color, -1)
            cv2.rectangle(hidden, (xmin, ymax), (xmax, image_height), color, -1)
            mask = hidden.astype(bool)
            image[mask] = cv2.addWeighted(image, alpha, hidden, 1-alpha, 0)[mask]
            # Show
            cv2.imshow('Hand controller preview', image)
            if cv2.waitKey(5) & 0xFF == 27:
                break
                    
capture.release()
