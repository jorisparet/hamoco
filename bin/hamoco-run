#!/usr/bin/env python

#TODO: adapt certain elements to the current framerate
#TODO: more accurate mouse movement with close fist
#TODO: relative pointer movement instead of absolute
#TODO: struggle to go from open to index up?
#TODO: put all the controller-related actions somewhere else
#TODO: Add a global description to the parser

import argparse

import cv2
import mediapipe as mp
import pyautogui
import keras
import numpy

from hamoco import Hand, HandyMouseController
from hamoco.models import __default_model__
from hamoco.utils import draw_hand_landmarks, draw_palm_center, draw_control_bounds, __window_name__

# Mediapipe shortcuts
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

# Allow mouse to go on the border of the screen
pyautogui.FAILSAFE = False

# Parser

parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
description = f"""{parser.prog} activates the webcam and allows to use hand
gestures to take control of the mouse pointer. Several basic actions can then
be performed, such as left click, right click, drag and drop and scrolling.""".replace('\n',' ')
parser.description = description
parser.add_argument('-S', '--sensitivity', 
                    default=0.25, 
                    type=float, 
                    help='Mouse sensitivity (between 0 and 1)')
parser.add_argument('--scrolling_threshold', 
                    default=0.1, 
                    type=float, 
                    help='Amount of change from the origin scrolling position to start scrolling (between 0 and 1)')
parser.add_argument('--min_cutoff_filter', 
                    default=0.1, 
                    type=float, 
                    help='Minimum cutoff for motion smoothing (low values decrease jitter but increase speed lag)')
parser.add_argument('--minimum_prediction_confidence',
                    default=0.8, 
                    type=float, 
                    help='Minimum prediction confidence for predicting hand poses')
parser.add_argument('-m', '--model', 
                    default=None, 
                    type=str,
                    help='Path to the Keras model to use for hand pose prediction')
parser.add_argument('--show',
                    action='store_true',
                    help='Real-time display of the processed camera feed')
args = parser.parse_args()
# Custom variables linked to parser
sensitivity = args.sensitivity
min_cutoff_filter = args.min_cutoff_filter
scrolling_threshold = args.scrolling_threshold
minimum_prediction_confidence = args.minimum_prediction_confidence
model = args.model
show_feed = args.show

# Load classification model
path_to_model = __default_model__ if model is None else model
trained_model = keras.models.load_model(path_to_model)

# Hand controller
hand_controller = HandyMouseController(sensitivity=sensitivity,
                                       scrolling_threshold=scrolling_threshold,
                                       min_cutoff_filter=min_cutoff_filter)

# Webcam input
capture = cv2.VideoCapture(0)
with mp_hands.Hands(static_image_mode=False,
                    model_complexity=1,
                    max_num_hands=1,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5) as hands:

    # Detect hand movement while the video capture is on
    while capture.isOpened():
        success, image = capture.read()
        image = cv2.flip(image, 1)

        if not success:
            print('Ignoring empty camera frame.')
            # If loading a video, use 'break' instead of 'continue'.
            continue

        # A hand is detected
        results = hands.process(image)
        hand_detected = bool(results.multi_hand_landmarks)
        if hand_detected:

            # Draw the hand annotations on the image
            draw_hand_landmarks(image, results.multi_hand_landmarks[0])

            # Landmark coordinates of the detected hand
            landmarks = results.multi_hand_landmarks[0].landmark
            hand = Hand()
            raw_landmark_vector = hand.vectorize_landmarks(landmarks)
            palm_center = hand_controller.palm_center(raw_landmark_vector)

            # Predict hand pose
            processed_landmark_vector = hand.feature_process_landmarks(raw_landmark_vector)
            probabilities = trained_model.predict(processed_landmark_vector).flatten()
            prediction_confidence = numpy.max(probabilities)
            predicted_pose = numpy.argmax(probabilities)
            hand.pose = Hand.Pose(predicted_pose)

            # Perform the appropriate mouse action
            hand_controller.operate_mouse(hand, 
                                          palm_center,
                                          prediction_confidence,
                                          min_confidence=minimum_prediction_confidence)

        # Show the camera feed
        if show_feed:

            # Show palm center
            if hand_detected:
                draw_palm_center(image, palm_center, size=20)

            # Accessible area        
            bounds = hand_controller.accessible_area(image)
            draw_control_bounds(image, bounds)

            # Show
            cv2.imshow(__window_name__, image)
            if cv2.waitKey(5) & 0xFF == 27:
                break
                    
capture.release()
