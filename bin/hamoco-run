#!/usr/bin/env python

#TODO: adapt certain elements to the current framerate
#TODO: more accurate mouse movement with close fist
#TODO: relative pointer movement instead of absolute
#TODO: struggle to go from open to index up?
#TODO: put all the controller-related actions somewhere else
#TODO: Add a global description to the parser

import time
import argparse

import cv2
import mediapipe as mp
import pyautogui
import keras
import numpy

from hamoco import Hand, HandyMouseController, OneEuroFilter
from hamoco.models import __default_model__

# Mediapipe shortcuts
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_hands = mp.solutions.hands

# Parser
parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
parser.add_argument('-s', '--sensitivity', 
                    default=0.25, 
                    type=float, 
                    help='Mouse sensitivity (between 0 and 1)')
parser.add_argument('--scrolling_threshold', 
                    default=0.1, 
                    type=float, 
                    help='Amount of change from the origin scrolling position to start scrolling (between 0 and 1)')
parser.add_argument('--minimum_prediction_confidence',
                    default=0.8, 
                    type=float, 
                    help='Minimum prediction confidence for predicting hand poses')
parser.add_argument('-m', '--model', 
                    default=None, 
                    type=str,
                    help='Path to the Keras model to use for hand pose prediction')
parser.add_argument('--show',
                    action='store_true',
                    help='Real-time display of the processed camera feed')
args = parser.parse_args()
# Custom variables linked to parser
sensitivity = args.sensitivity
scrolling_threshold = args.scrolling_threshold
minimum_prediction_confidence = args.minimum_prediction_confidence
model = args.model
show_feed = args.show

# Load classification model
path_to_model = __default_model__ if model is None else model
trained_model = keras.models.load_model(path_to_model)

# Allow mouse to go on the border of the screen
pyautogui.FAILSAFE = False

# Hand controller
hand_controller = HandyMouseController(sensitivity=sensitivity)

# Signal filter
min_cutoff = 0.1
filter_x = OneEuroFilter(0, 0.0, min_cutoff=min_cutoff)
filter_y = OneEuroFilter(0, 0.0, min_cutoff=min_cutoff)

# Webcam input
capture = cv2.VideoCapture(0)
with mp_hands.Hands(static_image_mode=False,
                    model_complexity=1,
                    max_num_hands=1,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5) as hands:

    # Sample click
    last_click_sample_time = time.time()
    previous_hand_pose = -1

    # Detect hand movement while the video capture is on
    frame = 0
    while capture.isOpened():
        frame += 1
        success, image = capture.read()
        image_height, image_width, _ = image.shape
        image = cv2.flip(image, 1)

        if not success:
            print('Ignoring empty camera frame.')
            # If loading a video, use 'break' instead of 'continue'.
            continue

        # A hand is detected
        results = hands.process(image)
        if results.multi_hand_landmarks:

            # Draw the hand annotations on the image
            # We only look at the first detected hand
            image.flags.writeable = True
            mp_drawing.draw_landmarks(image, 
                results.multi_hand_landmarks[0], 
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())

            # Detected hand's landmarks coordinates
            hand = Hand()
            landmarks = results.multi_hand_landmarks[0].landmark
            landmark_vector = hand.process_landmarks(landmarks)

            # We compute the coordinates (x,y) of the center of mass of the hand
            #  be averaging the positions of the different landmarks
            # num_landmarks = len(landmarks)
            palm_landmarks = [0,5,9,13,17]
            num_landmarks = len(palm_landmarks)
            hand_center_x, hand_center_y = 0.0, 0.0
            # for landmark_index in range(num_landmarks):
            for landmark_index in palm_landmarks:
                hand_center_x += landmarks[landmark_index].x
                hand_center_y += landmarks[landmark_index].y
            hand_center_x = (1.0 / num_landmarks) * hand_center_x
            hand_center_y = (1.0 / num_landmarks) * hand_center_y
            hand_center_x = filter_x(frame, hand_center_x)
            hand_center_y = filter_y(frame, hand_center_y)
            hand_center = numpy.array([hand_center_x, hand_center_y])

            # Predict hand pose
            probabilities = trained_model.predict(landmark_vector.reshape(1,-1)).flatten()
            prediction_confidence = numpy.max(probabilities)
            predicted_pose = numpy.argmax(probabilities)
            hand.pose = Hand.Pose(predicted_pose)

            # Mouse in STANDARD mode
            if hand_controller.current_mouse_state == HandyMouseController.MouseState.STANDARD:

                # Move the mouse
                if hand.pose == HandyMouseController.Event.MOVE:
                    hand_controller.move_pointer_to_hand_world(hand_center)
                    previous_hand_pose = hand.pose

                # Hand pose changed: perform the appropriate action
                elif hand.pose != previous_hand_pose and prediction_confidence > minimum_prediction_confidence:
                    hand_controller._on_pose_change(hand.pose, frame)
                    previous_hand_pose = hand.pose
            
            # Mouse in SCROLLING mode
            elif hand_controller.current_mouse_state == HandyMouseController.MouseState.SCROLLING:

                # Scroll up or down
                if hand.pose == HandyMouseController.Event.SCROLL:

                    # Get the reference position (origin) when scrolling begins (first frame)
                    if hand_controller.current_state_init == frame - 1:
                        scrolling_origin_y = hand_center_y
                    # Scroll up/down if above/below a certain distance threshold with
                    #  respect to the scrolling origin point (next frames)
                    else:
                        diff_to_origin_y = scrolling_origin_y - hand_center_y
                        if abs(diff_to_origin_y) > scrolling_threshold:
                            pyautogui.scroll(numpy.sign(diff_to_origin_y), _pause=False)

                # Hand pose changed: perform the appropriate action
                elif hand.pose != previous_hand_pose and prediction_confidence > minimum_prediction_confidence:
                    hand_controller._on_pose_change(hand.pose, frame)
                    previous_hand_pose = hand.pose

            # Mouse in DRAGGING mode
            elif hand_controller.current_mouse_state == HandyMouseController.MouseState.DRAGGING:

                # Get the reference position (origin) when scrolling begins (first frame)
                if hand_controller.current_state_init == frame - 1:
                    pyautogui.mouseDown(_pause=False)
                elif hand.pose != previous_hand_pose and prediction_confidence > minimum_prediction_confidence:
                    pyautogui.mouseUp(_pause=False)
                    hand_controller._on_pose_change(hand.pose, frame)
                    previous_hand_pose = hand.pose
                else:
                    hand_controller.move_pointer_to_hand_world(hand_center)
                    previous_hand_pose = hand.pose

        # Show the camera feed
        if show_feed:
            # Hand center
            if results.multi_hand_landmarks:
                cursor_xy = (hand_center * numpy.array([image_width, image_height])).astype(int)
                cursor_size = 10
                cv2.rectangle(image, tuple(cursor_xy - cursor_size), 
                                    tuple(cursor_xy + cursor_size), 
                                    (255,255,255), 3)
            # Accessible area        
            xmin, ymin, xmax, ymax = hand_controller.accessible_area(image)
            hidden = numpy.zeros_like(image, numpy.uint8)
            alpha = 0.25
            color = (1,1,1)
            cv2.rectangle(hidden, (0, 0), (xmin, image_height), color, -1)
            cv2.rectangle(hidden, (xmin, 0), (xmax, ymin), color, -1)
            cv2.rectangle(hidden, (xmax, 0), (image_width, image_height), color, -1)
            cv2.rectangle(hidden, (xmin, ymax), (xmax, image_height), color, -1)
            mask = hidden.astype(bool)
            image[mask] = cv2.addWeighted(image, alpha, hidden, 1-alpha, 0)[mask]
            # Show
            cv2.imshow('Hand controller preview', image)
            if cv2.waitKey(5) & 0xFF == 27:
                break
                    
capture.release()
